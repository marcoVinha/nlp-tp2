{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set value of CUDA_VISIBLE_DEVICES=\"\" to disable GPU\n",
    "import os\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = \"nilc-nlp/mac_morpho\"\n",
    "model_name = \"neuralmind/bert-base-portuguese-cased\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load dataset from HuggingFace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/marcovinha/nlp-tp2/.venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(dataset_name, trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Label mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'PREP+PROADJ': 0,\n",
       " 'IN': 1,\n",
       " 'PREP+PRO-KS': 2,\n",
       " 'NPROP': 3,\n",
       " 'PREP+PROSUB': 4,\n",
       " 'KC': 5,\n",
       " 'PROPESS': 6,\n",
       " 'NUM': 7,\n",
       " 'PROADJ': 8,\n",
       " 'PREP+ART': 9,\n",
       " 'KS': 10,\n",
       " 'PRO-KS': 11,\n",
       " 'ADJ': 12,\n",
       " 'ADV-KS': 13,\n",
       " 'N': 14,\n",
       " 'PREP': 15,\n",
       " 'PROSUB': 16,\n",
       " 'PREP+PROPESS': 17,\n",
       " 'PDEN': 18,\n",
       " 'V': 19,\n",
       " 'PREP+ADV': 20,\n",
       " 'PCP': 21,\n",
       " 'CUR': 22,\n",
       " 'ADV': 23,\n",
       " 'PU': 24,\n",
       " 'ART': 25}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get unique labels\n",
    "labels = dataset[\"train\"].features[\"pos_tags\"].feature.names\n",
    "\n",
    "# Create a mapping\n",
    "label2id = {label: i for i, label in enumerate(labels)}\n",
    "id2label = {i: label for label, i in label2id.items()}\n",
    "\n",
    "label2id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load from HuggingFace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_name,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenize dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_and_align_labels(examples):\n",
    "    tokenized_inputs = tokenizer(\n",
    "        examples[\"tokens\"], truncation=True, padding=True, is_split_into_words=True\n",
    "    )\n",
    "\n",
    "    labels = []\n",
    "    for i, label in enumerate(examples[\"pos_tags\"]):\n",
    "        word_ids = tokenized_inputs.word_ids(batch_index=i)  # Map tokens to words\n",
    "        aligned_labels = []\n",
    "        previous_word_id = None\n",
    "        for word_id in word_ids:\n",
    "            if word_id is None:\n",
    "                aligned_labels.append(-100)  # Ignore special tokens\n",
    "            elif word_id != previous_word_id:  # Start of a new word\n",
    "                aligned_labels.append(label[word_id])\n",
    "            else:\n",
    "                aligned_labels.append(-100)  # Ignore subword tokens\n",
    "            previous_word_id = word_id\n",
    "        labels.append(aligned_labels)\n",
    "\n",
    "    tokenized_inputs[\"labels\"] = labels\n",
    "    return tokenized_inputs\n",
    "\n",
    "\n",
    "# Tokenize the dataset\n",
    "tokenized_dataset = dataset.map(tokenize_and_align_labels, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForTokenClassification\n",
    "\n",
    "data_collator = DataCollatorForTokenClassification(tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine tune model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at neuralmind/bert-base-portuguese-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForTokenClassification\n",
    "\n",
    "\n",
    "model = AutoModelForTokenClassification.from_pretrained(\n",
    "    model_name, num_labels=len(label2id), id2label=id2label, label2id=label2id\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine tune model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining custom metrics to evaluate model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score\n",
    "import numpy as np\n",
    "\n",
    "def compute_metrics(p):\n",
    "    predictions, labels = p\n",
    "\n",
    "    # Get predicted labels by taking argmax\n",
    "    predictions = np.argmax(predictions, axis=-1)\n",
    "    \n",
    "    # Flatten and filter out ignored tokens (-100)\n",
    "    true_labels = labels.flatten()\n",
    "    pred_labels = predictions.flatten()\n",
    "    mask = true_labels != -100\n",
    "    true_labels = true_labels[mask]\n",
    "    pred_labels = pred_labels[mask]\n",
    "    \n",
    "    # Compute precision\n",
    "    macro_precision = precision_score(true_labels, pred_labels, average=\"macro\", zero_division=0)\n",
    "    weighted_precision = precision_score(true_labels, pred_labels, average=\"weighted\", zero_division=0)\n",
    "    per_class_precision = precision_score(true_labels, pred_labels, average=None, zero_division=0)\n",
    "\n",
    "    # Map class indices to precision values\n",
    "    unique_tags = np.unique(true_labels)\n",
    "    per_class_precision_dict = {id2label[int(tag)]: float(per_class_precision[i]) for i, tag in enumerate(unique_tags)}\n",
    "    \n",
    "    return {\n",
    "        \"macro_precision\": macro_precision,\n",
    "        \"weighted_precision\": weighted_precision,\n",
    "        \"per_class_precision\": per_class_precision_dict,\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/marcovinha/nlp-tp2/.venv/lib/python3.10/site-packages/transformers/training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import TrainingArguments, Trainer\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\".results\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=5e-5,\n",
    "    num_train_epochs=3,\n",
    "    save_strategy=\"epoch\",\n",
    "    use_cpu=True,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset[\"train\"],\n",
    "    eval_dataset=tokenized_dataset[\"validation\"],\n",
    "    data_collator=data_collator,\n",
    "    processing_class=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Against test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1249' max='1249' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1249/1249 27:34]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 3.2472450733184814, 'eval_model_preparation_time': 0.004, 'eval_macro_precision': 0.039374073146533455, 'eval_weighted_precision': 0.1169947038889422, 'eval_per_class_precision': {'PREP+PROADJ': 0.001557026080186843, 'IN': 0.00022401433691756272, 'PREP+PRO-KS': 0.0009269988412514484, 'NPROP': 0.03873574144486692, 'PREP+PROSUB': 0.0015748031496062992, 'KC': 0.04230694887406978, 'PROPESS': 0.0006574621959237344, 'NUM': 0.038202247191011236, 'PROADJ': 0.005723630417007359, 'PREP+ART': 0.03877908431323493, 'KS': 0.0026536930561698365, 'PRO-KS': 0.018703898840885143, 'ADJ': 0.09069981583793739, 'ADV-KS': 0.0004393673110720562, 'N': 0.3588334742180896, 'PREP': 0.12414823112106786, 'PROSUB': 0.017066233238520925, 'PREP+PROPESS': 0.002097535395909806, 'PDEN': 0.0, 'V': 0.12671905697445973, 'PREP+ADV': 0.00044583147570218456, 'PCP': 0.015246015246015246, 'CUR': 0.0, 'ADV': 0.06662553979025293, 'PU': 0.013074969429028313, 'ART': 0.01828428303068253}, 'eval_runtime': 1656.9818, 'eval_samples_per_second': 6.027, 'eval_steps_per_second': 0.754}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('./fine_tuned_model/tokenizer_config.json',\n",
       " './fine_tuned_model/special_tokens_map.json',\n",
       " './fine_tuned_model/vocab.txt',\n",
       " './fine_tuned_model/added_tokens.json',\n",
       " './fine_tuned_model/tokenizer.json')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = trainer.evaluate(\n",
    "    # tokenized_dataset[\"test\"].shuffle(seed=42).select(range(0, 10))\n",
    "    tokenized_dataset[\"test\"]\n",
    ")\n",
    "print(results)\n",
    "\n",
    "model.save_pretrained(\"./fine_tuned_model\")\n",
    "tokenizer.save_pretrained(\"./fine_tuned_model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Against another model trained on the same dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertForTokenClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(29794, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSdpaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=27, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load `lisaterumi/postagger-portuguese` from hugging face and store in a `competitor_model` variable\n",
    "\n",
    "from transformers import BertForTokenClassification\n",
    "\n",
    "competitor_model = BertForTokenClassification.from_pretrained(\n",
    "    \"lisaterumi/postagger-portuguese\"\n",
    ")\n",
    "\n",
    "competitor_tokenizer = AutoTokenizer.from_pretrained(\"lisaterumi/postagger-portuguese\")\n",
    "\n",
    "competitor_model.to(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1997/1997 [00:00<00:00, 4166.36 examples/s]\n"
     ]
    }
   ],
   "source": [
    "def tokenize_and_align_labels_for_comparison(examples):\n",
    "    tokenized_inputs = competitor_tokenizer(\n",
    "        examples[\"tokens\"],\n",
    "        truncation=True,\n",
    "        is_split_into_words=True,\n",
    "        padding=\"max_length\",\n",
    "        max_length=128,\n",
    "    )\n",
    "\n",
    "    # Align labels with tokenized inputs\n",
    "    labels = []\n",
    "    for i, label in enumerate(examples[\"labels\"]):\n",
    "        word_ids = tokenized_inputs.word_ids(batch_index=i)\n",
    "        aligned_labels = [\n",
    "            -100 if word_id is None else label[word_id] for word_id in word_ids\n",
    "        ]\n",
    "        labels.append(aligned_labels)\n",
    "\n",
    "    tokenized_inputs[\"labels\"] = labels\n",
    "    return tokenized_inputs\n",
    "\n",
    "\n",
    "tokenized_test_dataset_comparison = tokenized_dataset.map(\n",
    "    tokenize_and_align_labels_for_comparison, batched=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/marcovinha/nlp-tp2/.venv/lib/python3.10/site-packages/transformers/training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the `competitor_model` using the same `compute_metrics` function and the same `tokenized_dataset[\"test\"]` dataset\n",
    "\n",
    "# Define dummy training arguments (just for evaluation)\n",
    "dummy_training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    logging_dir=\"./logs\",\n",
    "    do_train=False,\n",
    "    do_eval=True,\n",
    "    evaluation_strategy=\"no\",\n",
    ")\n",
    "\n",
    "# Create a Trainer for the comparison model\n",
    "competitor_trainer = Trainer(\n",
    "    model=competitor_model,\n",
    "    args=training_args,\n",
    "    eval_dataset=tokenized_test_dataset_comparison[\"validation\"],\n",
    "    processing_class=competitor_tokenizer,\n",
    "    compute_metrics=compute_metrics,  # Use the same custom metric function\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1249' max='1249' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1249/1249 22:49]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 10.260217666625977, 'eval_model_preparation_time': 0.0035, 'eval_macro_precision': 0.034122055046705034, 'eval_weighted_precision': 0.07739469736230463, 'eval_per_class_precision': {'PREP+PROADJ': 0.0, 'IN': 0.0005425935973955507, 'PREP+PRO-KS': 0.0, 'NPROP': 0.06532663316582915, 'PREP+PROSUB': 0.0005937654626422563, 'KC': 0.05448717948717949, 'PROPESS': 0.01650038372985418, 'NUM': 0.011442141623488774, 'PROADJ': 0.018442622950819672, 'PREP+ART': 0.05470727180698498, 'KS': 0.009498564170532362, 'PRO-KS': 0.022401433691756272, 'ADJ': 0.049019607843137254, 'ADV-KS': 0.002431681333950903, 'N': 0.1430054848500985, 'PREP': 0.08588692274492879, 'PROSUB': 0.08333333333333333, 'PREP+PROPESS': 0.0, 'PDEN': 0.0050009805844283195, 'V': 0.0761904761904762, 'PREP+ADV': 0.0, 'PCP': 0.006422018348623854, 'CUR': 0.0, 'ADV': 0.058823529411764705, 'PU': 0.12311680688710624, 'ART': 0.0}, 'eval_runtime': 1371.498, 'eval_samples_per_second': 7.282, 'eval_steps_per_second': 0.911}\n"
     ]
    }
   ],
   "source": [
    "eval_results_comparison = competitor_trainer.evaluate(\n",
    "    tokenized_test_dataset_comparison[\"test\"]\n",
    ")\n",
    "\n",
    "print(eval_results_comparison)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
