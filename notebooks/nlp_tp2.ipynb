{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine tuning BERTimbau to perform Parts of Speech Tagging\n",
    "\n",
    "This notebook fine tunes a BERTimbau model (`neuralmind/bert-base-portuguese-cased`, 110M parameters) in the Mac-Morpho dataset to perform a Parts of Speech Tagging (POS Tagging) task in portuguese.\n",
    "\n",
    "The evaluation of this model's performance consists in:\n",
    "- Calculating three different metrics on the test dataset:\n",
    "    - Macro precision\n",
    "    - Weighted precision\n",
    "    - Per-class precision\n",
    "- Comparing these metrics with the ones obtained by a competitor model:\n",
    "    - The competitor model is the `lisaterumi/postagger-portuguese`\n",
    "    - This model is also a BERTimbau-based model\n",
    "    - This model is also fine tuned to perform POS Tagging in the Mac-Morpho dataset\n",
    "\n",
    "In practice, the model we'll fine tune aims to reproduce the results of the `lisaterumi/postagger-portuguese` model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = \"nilc-nlp/mac_morpho\"\n",
    "model_name = \"neuralmind/bert-base-portuguese-cased\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The dataset\n",
    "\n",
    "From the official Mac-Morpho [web-page](http://nilc.icmc.usp.br/macmorpho/):\n",
    "\n",
    "> Mac-Morpho is a corpus of Brazilian Portuguese texts annotated with part-of-speech tags. Its first version was released in 2003, and since then, two revisions have been made in order to improve the quality of the resource.\n",
    "\n",
    "According to the [dataset manual](http://nilc.icmc.usp.br/macmorpho/macmorpho-manual.pdf), it is already split in a train, a validation and a test sub-sets, and it contains a total of 27 possible classes/tags. The following table shows the classes in our dataset:\n",
    "\n",
    "| Tag |  Meaning (grammatical class in portuguese)|\n",
    "| ------------------- | ------------------- |\n",
    "|  ADJ |  Adjetivo |\n",
    "|  ADV |  Advérbio |\n",
    "|  ADV-KS |  Advérbio conjuntivo subordinado  |\n",
    "|  ADV-KS-REL |   Advérbio relativo subordinado |\n",
    "|  ART |  Artigo  |\n",
    "|  CUR |  Moeda  |\n",
    "|  IN |  Interjeição |\n",
    "|  KC |  Conjunção coordenativa |\n",
    "|  KS |  Conjunção subordinativa |\n",
    "|  N |  Substantivo |\n",
    "|  NPROP | Substantivo próprio |\n",
    "|  NUM |  Número |\n",
    "|  PCP |  Particípio |\n",
    "|  PDEN |  Palavra denotativa |\n",
    "|  PREP |  Preposição |\n",
    "|  PROADJ |  Pronome Adjetivo |\n",
    "|  PRO-KS |  Pronome conjuntivo subordinado |\n",
    "|  PRO-KS-REL |  Pronome relativo conectivo subordinado |\n",
    "|  PROPESS |  Pronome pessoal |\n",
    "|  PROSUB |  Pronome nominal |\n",
    "|  V | Verbo |\n",
    "|  VAUX  | Verbo auxiliar |\n",
    "\n",
    "Next, we'll prepare the dataset for the fine tuning task.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load dataset from HuggingFace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(dataset_name, trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating maps from label to ID and vice-versa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get unique labels\n",
    "labels = dataset[\"train\"].features[\"pos_tags\"].feature.names\n",
    "\n",
    "# Create the mappings\n",
    "label2id = {label: i for i, label in enumerate(labels)}\n",
    "id2label = {i: label for label, i in label2id.items()}\n",
    "\n",
    "label2id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the BERTimbau tokenizer from HuggingFace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_name,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForTokenClassification\n",
    "\n",
    "data_collator = DataCollatorForTokenClassification(tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenize and align dataset\n",
    "\n",
    "The tokenization process (transforming words/tokens in numbers) is an already known sub-task in NLP tasks, so we'll skip any explanations. But one additional thing we'll do is **aligning** the labels with the correct tokens we want to classify.\n",
    "\n",
    "Aligning, in this context, means *knowing what subwords/tokens of a word we want to tag*. When you tokenize a sentence, especially using subword tokenizers (which is the case with the tokenizer we're using), the sentence is split into subword tokens. Because of this, we need to ensure that the labels are correctly associated with these tokens. This is where label alignment comes into play.\n",
    "\n",
    "In our dataset, each word has a single label, but tokenization may result in subwords. In the end of our tokenization process, the number of tokens often exceeds the number of original words. For example:\n",
    "- Imagine our labels look something like: `label2id = {\"NOUN\": 0, \"VERB\": 1, ...}`\n",
    "- Now imagine that we're trying to classify the word `playing` into a `VERB`\n",
    "- When we tokenized THE word, it is broken into **two tokens**: `[\"play\", \"ing\"]`\n",
    "- What should we tag as a `VERB`: `play`, or `ing`?\n",
    "\n",
    "In this case, we'll fix this by \"aligning\" our dataset:\n",
    "1. The first token of a word (in our case, `play`) will be tagged with the original word's label (in our case, `VERB`, with value `1`).\n",
    "2. The following tokens, will be ignored if we assign a special value/class to it. Typically, when we assign the value of `-100`, our model is already able to ignore the token, excluding it from the loss function calculation during training.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_and_align_labels(examples):\n",
    "    tokenized_inputs = tokenizer(\n",
    "        examples[\"tokens\"], truncation=True, padding=True, is_split_into_words=True\n",
    "    )\n",
    "\n",
    "    labels = []\n",
    "    for i, label in enumerate(examples[\"pos_tags\"]):\n",
    "        word_ids = tokenized_inputs.word_ids(batch_index=i)\n",
    "        aligned_labels = []\n",
    "        previous_word_id = None\n",
    "        for word_id in word_ids:\n",
    "            if word_id is None:\n",
    "                aligned_labels.append(-100)\n",
    "            elif word_id != previous_word_id:\n",
    "                aligned_labels.append(label[word_id])\n",
    "            else:\n",
    "                aligned_labels.append(-100)\n",
    "            previous_word_id = word_id\n",
    "        labels.append(aligned_labels)\n",
    "\n",
    "    tokenized_inputs[\"labels\"] = labels\n",
    "    return tokenized_inputs\n",
    "\n",
    "\n",
    "tokenized_dataset = dataset.map(tokenize_and_align_labels, batched=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine tune our model\n",
    "\n",
    "We'll fine tune our model for only $5$ epochs, and we'll use a learning rate of $5\\mathrm{e}{-5}$. Our dataset is split in train (37948 samples), validation (1997 samples) and test (9987 samples) sub-sets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download model from HuggingFace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForTokenClassification\n",
    "\n",
    "\n",
    "model = AutoModelForTokenClassification.from_pretrained(\n",
    "    model_name, num_labels=len(label2id), id2label=id2label, label2id=label2id\n",
    ")\n",
    "\n",
    "model.to(\"cuda\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define fine tuning routine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining custom metrics to evaluate model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "import numpy as np\n",
    "\n",
    "def compute_metrics(p):\n",
    "    predictions, labels = p\n",
    "\n",
    "    # Get predicted labels by taking argmax\n",
    "    predictions = np.argmax(predictions, axis=-1)\n",
    "\n",
    "    # Flatten and filter out ignored tokens (-100)\n",
    "    true_labels = labels.flatten()\n",
    "    pred_labels = predictions.flatten()\n",
    "    mask = true_labels != -100\n",
    "    true_labels = true_labels[mask]\n",
    "    pred_labels = pred_labels[mask]\n",
    "\n",
    "    # Compute metrics\n",
    "    macro_precision = precision_score(\n",
    "        true_labels, pred_labels, average=\"macro\", zero_division=0\n",
    "    )\n",
    "    weighted_precision = precision_score(\n",
    "        true_labels, pred_labels, average=\"weighted\", zero_division=0\n",
    "    )\n",
    "    macro_recall = recall_score(\n",
    "        true_labels, pred_labels, average=\"macro\", zero_division=0\n",
    "    )\n",
    "    weighted_recall = recall_score(\n",
    "        true_labels, pred_labels, average=\"weighted\", zero_division=0\n",
    "    )\n",
    "    f1 = f1_score(true_labels, pred_labels, average=\"weighted\", zero_division=0)\n",
    "    per_class_precision = precision_score(\n",
    "        true_labels, pred_labels, average=None, zero_division=0\n",
    "    )\n",
    "\n",
    "    # Map class indices to precision values\n",
    "    unique_tags = np.unique(true_labels)\n",
    "    per_class_precision_dict = {\n",
    "        id2label[int(tag)]: float(per_class_precision[i])\n",
    "        for i, tag in enumerate(unique_tags)\n",
    "    }\n",
    "\n",
    "    return {\n",
    "        \"macro_precision\": macro_precision,\n",
    "        \"weighted_precision\": weighted_precision,\n",
    "        \"macro_recall\": macro_recall,\n",
    "        \"weighted_recall\": weighted_recall,\n",
    "        \"f1_score\": f1,\n",
    "        \"per_class_precision\": per_class_precision_dict,\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments, Trainer\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\".results\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=5e-5,\n",
    "    num_train_epochs=5,\n",
    "    save_strategy=\"epoch\",\n",
    "    use_cpu=False,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset[\"train\"],\n",
    "    eval_dataset=tokenized_dataset[\"validation\"],\n",
    "    data_collator=data_collator,\n",
    "    processing_class=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "\n",
    "def plot_class_precisions(per_class_precision):\n",
    "    \"\"\"\n",
    "    Plots a bar chart of precision scores for each class.\n",
    "\n",
    "    Args:\n",
    "    - per_class_precision (dict): Dictionary mapping class names (e.g., \"NOUN\", \"VERB\") to precision scores.\n",
    "\n",
    "    \"\"\"\n",
    "    # Extract class names and precision scores\n",
    "    class_names = list(per_class_precision.keys())\n",
    "    precision_scores = list(per_class_precision.values())\n",
    "\n",
    "    # Sort by precision (descending order)\n",
    "    sorted_indices = np.argsort(precision_scores)[::-1]\n",
    "    sorted_class_names = [class_names[i] for i in sorted_indices]\n",
    "    sorted_precisions = [precision_scores[i] for i in sorted_indices]\n",
    "\n",
    "    # Plot bar chart\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.bar(sorted_class_names, sorted_precisions, color=\"skyblue\", edgecolor=\"black\")\n",
    "\n",
    "    # Add labels and title\n",
    "    plt.xlabel(\"Classes\", fontsize=12)\n",
    "    plt.ylabel(\"Precision\", fontsize=12)\n",
    "    plt.title(\"Precision per Class (Sorted)\", fontsize=12)\n",
    "    plt.xticks(rotation=45, ha=\"right\", fontsize=10)\n",
    "    plt.ylim(0, 1)  # Precision values are between 0 and 1\n",
    "\n",
    "    # Annotate the bars with precision values\n",
    "    for i, precision in enumerate(sorted_precisions):\n",
    "        precision_shown = math.floor(precision * 100) / 100\n",
    "        plt.text(i, precision, f\"{precision_shown:.2f}\", ha=\"center\", fontsize=8)\n",
    "\n",
    "    # Display the plot\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def plot_class_distribution(dataset, label_column, id2label, threshold_percentage=1.0):\n",
    "    \"\"\"\n",
    "    Plots a pie chart of the class distribution in a given dataset where the labels are lists of lists,\n",
    "    showing the actual class names using `id2label`, with classes below a certain threshold percentage\n",
    "    grouped into an \"Other\" category.\n",
    "\n",
    "    Args:\n",
    "    - dataset (Dataset): The dataset to analyze (HuggingFace Dataset).\n",
    "    - label_column (str): The column name in the dataset that contains the class labels (lists of lists).\n",
    "    - id2label (dict): A dictionary mapping class IDs (numeric values) to class names (text labels).\n",
    "    - threshold_percentage (float): The percentage threshold below which classes will be grouped into the \"Other\" category.\n",
    "    \"\"\"\n",
    "    # Extract the labels (lists of labels) from the dataset\n",
    "    labels = dataset[label_column]\n",
    "\n",
    "    # Flatten the list of lists to count all occurrences of the individual labels\n",
    "    flat_labels = [label for sublist in labels for label in sublist]\n",
    "\n",
    "    # Map class IDs to class names using `id2label`\n",
    "    class_names = [id2label[label] for label in flat_labels]\n",
    "\n",
    "    # Count the occurrences of each class\n",
    "    class_counts = {\n",
    "        class_name: class_names.count(class_name) for class_name in set(class_names)\n",
    "    }\n",
    "\n",
    "    # Calculate the total number of labels\n",
    "    total_labels = len(flat_labels)\n",
    "\n",
    "    # Sort classes by frequency (most frequent first)\n",
    "    sorted_class_counts = dict(\n",
    "        sorted(class_counts.items(), key=lambda item: item[1], reverse=True)\n",
    "    )\n",
    "\n",
    "    # Identify classes with percentage below the threshold\n",
    "    threshold_count = total_labels * (threshold_percentage / 100)\n",
    "\n",
    "    # Group classes below the threshold percentage into 'Other'\n",
    "    other_count = 0\n",
    "    filtered_class_counts = {}\n",
    "    for class_name, count in sorted_class_counts.items():\n",
    "        if count < threshold_count:\n",
    "            other_count += count\n",
    "        else:\n",
    "            filtered_class_counts[class_name] = count\n",
    "\n",
    "    # Add 'Other' category if there are any classes below the threshold\n",
    "    if other_count > 0:\n",
    "        filtered_class_counts[\"Other\"] = other_count\n",
    "\n",
    "    # Prepare data for pie chart\n",
    "    sorted_class_names = list(filtered_class_counts.keys())\n",
    "    sorted_counts = list(filtered_class_counts.values())\n",
    "\n",
    "    # Plot pie chart\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    wedges, texts, autotexts = plt.pie(\n",
    "        sorted_counts,\n",
    "        labels=sorted_class_names,\n",
    "        autopct=\"%1.1f%%\",\n",
    "        startangle=90,\n",
    "        colors=plt.cm.Paired.colors,\n",
    "        textprops={\"fontsize\": 10},\n",
    "        wedgeprops={\"edgecolor\": \"black\"},\n",
    "    )\n",
    "\n",
    "    # Make the labels and percentages more readable by adjusting the label distance and font size\n",
    "    for text in texts + autotexts:\n",
    "        text.set_fontsize(10)\n",
    "\n",
    "    # Rotate the labels for better readability\n",
    "    plt.setp(texts, rotation=45, ha=\"right\")\n",
    "\n",
    "    # Add a sorted legend with class names and percentages\n",
    "    plt.legend(\n",
    "        title=\"Classes\",\n",
    "        labels=[\n",
    "            f\"{name}: {count} ({count/total_labels*100:.1f}%)\"\n",
    "            for name, count in zip(sorted_class_names, sorted_counts)\n",
    "        ],\n",
    "        loc=\"upper left\",\n",
    "        fontsize=10,\n",
    "        bbox_to_anchor=(1.0, 1.0),\n",
    "    )\n",
    "\n",
    "    # Set the title and aspect ratio\n",
    "    plt.title(\"Class Distribution in Test Dataset\", fontsize=14)\n",
    "    plt.axis(\"equal\")  # Equal aspect ratio ensures that pie chart is drawn as a circle.\n",
    "\n",
    "    # Display the plot\n",
    "    plt.tight_layout()  # Adjust layout to prevent overlap\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Against test data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysing class distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_class_distribution(\n",
    "    tokenized_dataset[\"test\"],\n",
    "    label_column=\"pos_tags\",\n",
    "    id2label=id2label,\n",
    "    threshold_percentage=2.5,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyse model performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_results = trainer.evaluate(tokenized_dataset[\"test\"])\n",
    "\n",
    "model_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_class_precisions(model_results[\"eval_per_class_precision\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overall model performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparing to the competitor model\n",
    "\n",
    "The `lisaterumi/postagger-portuguese` model reports these metrics in its [HuggingFace page](https://huggingface.co/lisaterumi/postagger-portuguese):\n",
    "\n",
    "```\n",
    "              Precision  Recall      F1\n",
    "accuracy                           0.98\n",
    "macro avg          0.96    0.95    0.95\n",
    "weighted avg       0.98    0.98    0.98\n",
    "\n",
    "F1:  0.9826\n",
    "Accuracy:  0.9826\n",
    "```\n",
    "\n",
    "For the model we trained, these are the reports:\n",
    "\n",
    "```\n",
    "              Precision  Recall      F1\n",
    "accuracy                           0.\n",
    "macro avg          0.    0.    0.\n",
    "weighted avg       0.    0.    0.\n",
    "\n",
    "F1:  0.\n",
    "Accuracy:  0.\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating per-class precision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
